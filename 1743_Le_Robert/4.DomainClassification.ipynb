{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain classification\n",
    "\n",
    "The model is on HuggingFace: https://huggingface.co/GEODE/bert-base-multilingual-cased-edda-domain-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will use the GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# for MacOS\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print('We will use the GPU')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, TextClassificationPipeline\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataloader(tokenizer, sentences, batch_size = 8, max_len = 512):\n",
    "\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids_test = []\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "        # `encode` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            # This function also supports truncation and conversion\n",
    "                            # to pytorch tensors, but I need to do padding, so I\n",
    "                            # can't use these features.\n",
    "                            #max_length = max_len,          # Truncate all sentences.\n",
    "                            #return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        input_ids_test.append(encoded_sent)\n",
    "\n",
    "    # Pad our input tokens\n",
    "    padded_test = []\n",
    "    for i in input_ids_test:\n",
    "        if len(i) > max_len:\n",
    "            padded_test.extend([i[:max_len]])\n",
    "        else:\n",
    "            padded_test.extend([i + [0] * (max_len - len(i))])\n",
    "    input_ids_test = np.array(padded_test)\n",
    "\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids_test:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "\n",
    "    # Convert to tensors.\n",
    "    inputs = torch.tensor(input_ids_test)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    # Create the DataLoader.\n",
    "    data = TensorDataset(inputs, masks)\n",
    "    prediction_sampler = SequentialSampler(data)\n",
    "\n",
    "    return DataLoader(data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def data(dataset):\n",
    "    for d in dataset:\n",
    "        yield f\"{d}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Bert Tokenizer...\n",
      "Loading GEODE Classifier...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d8fc152cf34c9ca1de95728f27bb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = 'GEODE/bert-base-multilingual-cased-edda-domain-classification'\n",
    "\n",
    "print('Loading Bert Tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "print('Loading GEODE Classifier...')\n",
    "model = BertForSequenceClassification.from_pretrained(model_path).to(device.type)\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=None, device=device, batch_size=8)\n",
    "\n",
    "# https://stackoverflow.com/questions/67849833/how-to-truncate-input-in-the-huggingface-pipeline\n",
    "tokenizer_kwargs = {'padding':True, 'truncation':True, 'max_length':512}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>entry</th>\n",
       "      <th>entry_lemma</th>\n",
       "      <th>subordinate</th>\n",
       "      <th>subordinate_lemma</th>\n",
       "      <th>subordinate_domain</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A est la première Lettre de l'Alphabet Françoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C'est inutilement que la plupart des Grammairi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A se prononce du gozier, ce qui ne rend pas ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Le son de l'a est ordinairement un son clair. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Le son de l'a est un de ceux que les muets for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A devant un e, avec lequel il fait une diphton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A devant un i, ou devant un y, avec lequel il ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A devant o &amp; ne faisant qu'une même syllabe av...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A devant u se prononce comme un o, comme dans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>250000010</td>\n",
       "      <td>A</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A devant y a le meme son que devant i ; il fau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   volume      entry entry_lemma  subordinate subordinate_lemma  \\\n",
       "0       1  250000010           A            1               NaN   \n",
       "1       1  250000010           A            2               NaN   \n",
       "2       1  250000010           A            3               NaN   \n",
       "3       1  250000010           A            4               NaN   \n",
       "4       1  250000010           A            5               NaN   \n",
       "5       1  250000010           A            6               NaN   \n",
       "6       1  250000010           A            7               NaN   \n",
       "7       1  250000010           A            8               NaN   \n",
       "8       1  250000010           A            9               NaN   \n",
       "9       1  250000010           A           10               NaN   \n",
       "\n",
       "  subordinate_domain                                            content  \n",
       "0                NaN  A est la première Lettre de l'Alphabet Françoi...  \n",
       "1                NaN  C'est inutilement que la plupart des Grammairi...  \n",
       "2                NaN  A se prononce du gozier, ce qui ne rend pas ce...  \n",
       "3                NaN  Le son de l'a est ordinairement un son clair. ...  \n",
       "4                NaN  Le son de l'a est un de ceux que les muets for...  \n",
       "5                NaN  A devant un e, avec lequel il fait une diphton...  \n",
       "6                NaN  A devant un i, ou devant un y, avec lequel il ...  \n",
       "7                NaN  A devant o & ne faisant qu'une même syllabe av...  \n",
       "8                NaN  A devant u se prononce comme un o, comme dans ...  \n",
       "9                NaN  A devant y a le meme son que devant i ; il fau...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.path.join('..', 'data', '1743_LeRobert')\n",
    "df = pd.read_csv(os.path.join(path, 'Trevoux1743.tsv'), sep='\\t')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df['content'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "135004it [26:44, 84.14it/s] \n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "\n",
    "for out in tqdm(pipe(data(dataset), **tokenizer_kwargs)):\n",
    "    out = sorted(out, key=lambda d: d['score'], reverse=True)\n",
    "    #print(out[0]['label'], out[0]['score'], out[1]['label'], out[1]['score'], out[2]['label'], out[2]['score'])\n",
    "    pred.append([out[0]['label'], out[0]['score'], out[1]['label'], out[1]['score'], out[2]['label'], out[2]['score']])\n",
    "\n",
    "pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Religion', '0.5409356951713562', 'Belles-lettres',\n",
       "        '0.3419780135154724', 'Philosophie', '0.04260238632559776'],\n",
       "       ['Philosophie', '0.5389487743377686', 'Belles-lettres',\n",
       "        '0.37168973684310913', 'Religion', '0.02738337032496929'],\n",
       "       ['Philosophie', '0.9752424955368042', 'Belles-lettres',\n",
       "        '0.005087039899080992', 'Médecine', '0.0036539973225444555'],\n",
       "       ...,\n",
       "       ['Géographie', '0.9952837824821472', 'Histoire',\n",
       "        '0.0026258418802171946', 'Militaire', '0.0002585176262073219'],\n",
       "       ['Géographie', '0.7414860129356384', 'Histoire',\n",
       "        '0.10729145258665085', 'Histoire naturelle',\n",
       "        '0.08297926187515259'],\n",
       "       ['Géographie', '0.9985746145248413', 'Histoire',\n",
       "        '0.00023022794630378485', 'Militaire', '0.00012393835640978068']],\n",
       "      dtype='<U32')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['domainPred1'] = pred[:,0]\n",
    "df['domainProba1'] = pred[:,1]\n",
    "df['domainPred2'] = pred[:,2]\n",
    "df['domainProba2'] = pred[:,3]\n",
    "df['domainPred3'] = pred[:,4]\n",
    "df['domainProba3'] = pred[:,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(path,\"Trevoux1743_domPred.tsv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stage24-classification-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
